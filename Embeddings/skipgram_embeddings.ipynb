{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skipgram Model \n",
    "The purpose of this notebook is to provide a exmaple on how to train the data in a skipgram model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of Training Data\n",
    "First step is to preprocess the data. \n",
    "1. Remove punctuation\n",
    "2. Remove stopwords\n",
    "3. Apply steeming\n",
    "4. Remove low frequency words\n",
    "5. Everything to lowercase\n",
    "6. Tokenize words\n",
    "\n",
    "For purpose of this notebook, we will only apply fifth and fourth steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize words and apply lowercase. (most models are case sensitive)\n",
    "\n",
    "def tokenize(text):\n",
    "    # obtains tokens\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text.lower())\n",
    "\n",
    "def mapping(tokens):\n",
    "    word_to_id = dict()\n",
    "    id_to_word = dict()\n",
    "    \n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "        \n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "\n",
    "\n",
    "def generate_training_data(tokens, word_to_id, window_size):\n",
    "    N = len(tokens)\n",
    "    X, Y = [], []\n",
    "    \n",
    "    for i in range(N):\n",
    "        nbr_inds = list(range(max(0, i - window_size), i)) + \\\n",
    "                   list(range(i + 1, min(N, i + window_size + 1)))\n",
    "        \n",
    "        for j in nbr_inds:\n",
    "            X.append(word_to_id[tokens[i]])\n",
    "            Y.append(word_to_id[tokens[j]])\n",
    "        \n",
    "    X = np.array(X)\n",
    "    X = np.expand_dims(X, axis = 0)\n",
    "    Y = np.array(Y)\n",
    "    Y = np.expand_dims(Y, axis = 0)\n",
    "    \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 7,  7,  7,  0,  0,  0,  0,  3,  3,  3,  3,  3,  8,  8,  8,  8,\n",
       "          8,  8,  0,  0,  0,  0,  0,  0,  5,  5,  5,  5,  5,  5,  8,  8,\n",
       "          8,  8,  8,  8, 11, 11, 11, 11, 11, 11, 10, 10, 10, 10, 10, 10,\n",
       "          0,  0,  0,  0,  0,  0, 12, 12, 12, 12, 12, 12,  2,  2,  2,  2,\n",
       "          2,  2,  4,  4,  4,  4,  4,  4,  9,  9,  9,  9,  9,  6,  6,  6,\n",
       "          6,  1,  1,  1]]),\n",
       " array([[ 0,  3,  8,  7,  3,  8,  0,  7,  0,  8,  0,  5,  7,  0,  3,  0,\n",
       "          5,  8,  0,  3,  8,  5,  8, 11,  3,  8,  0,  8, 11, 10,  8,  0,\n",
       "          5, 11, 10,  0,  0,  5,  8, 10,  0, 12,  5,  8, 11,  0, 12,  2,\n",
       "          8, 11, 10, 12,  2,  4, 11, 10,  0,  2,  4,  9, 10,  0, 12,  4,\n",
       "          9,  6,  0, 12,  2,  9,  6,  1, 12,  2,  4,  6,  1,  2,  4,  9,\n",
       "          1,  4,  9,  6]]))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_training_data(tokens, word_to_id, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"The quick brown fox jumps over the lazy dog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'the': 0,\n",
       "  'lazy': 1,\n",
       "  'jumps': 2,\n",
       "  'fox': 3,\n",
       "  'brown': 4,\n",
       "  'dog': 5,\n",
       "  'over': 6,\n",
       "  'quick': 7},\n",
       " {0: 'the',\n",
       "  1: 'lazy',\n",
       "  2: 'jumps',\n",
       "  3: 'fox',\n",
       "  4: 'brown',\n",
       "  5: 'dog',\n",
       "  6: 'over',\n",
       "  7: 'quick'})"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenize(doc)\n",
    "mapping(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id, id_to_word = mapping(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = generate_training_data(tokens, word_to_id, 3)\n",
    "vocab_size = len(id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Y.shape[1]\n",
    "Y_one_hot = np.zeros((vocab_size, m))\n",
    "Y_one_hot[Y.flatten(), np.arange(m)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7, 4, 3, 0, 4, 3, 2, 0, 7, 3, 2, 6, 0, 7, 4, 2, 6, 0, 7, 4, 3, 6,\n",
       "        0, 1, 4, 3, 2, 0, 1, 5, 3, 2, 6, 1, 5, 2, 6, 0, 5, 6, 0, 1]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_wrd_emb(vocab_size, emb_size):\n",
    "    \"\"\"\n",
    "    vocab_size: int. vocabulary size of your corpus or training data\n",
    "    emb_size: int. word embedding size. How many dimensions to represent each vocabulary\n",
    "    \"\"\"\n",
    "    word_emb = np.random.randn(vocab_size, emb_size) * 0.01\n",
    "    return word_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_dense(input_size, output_size):\n",
    "    \"\"\"\n",
    "    input_size: int. size of the input to the dense layer\n",
    "    output_size: int. size of the output of the dense layer\n",
    "    \"\"\"\n",
    "    W = np.random.randn(output_size, input_size) * 0.01\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(vocab_size, emb_size):\n",
    "    \"\"\"\n",
    "    initialize all the training parameters\n",
    "    \"\"\"\n",
    "    word_emb = initialize_wrd_emb(vocab_size, emb_size)\n",
    "    W = initialize_dense(emb_size, vocab_size)\n",
    "    \n",
    "    parameters = {}\n",
    "    parameters['word_emb'] = word_emb\n",
    "    parameters['W'] = W\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:24: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:24: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:24: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<ipython-input-73-b26b3764ab99>:24: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(Z.shape == W.shape[0], m)\n"
     ]
    }
   ],
   "source": [
    "## Propagation\n",
    "def ind_to_word_vecs(inds, parameters):\n",
    "    \"\"\"\n",
    "    inds: numpy array. shape: (1, m)\n",
    "    parameters: dict weights to be trained\n",
    "    \"\"\"\n",
    "    m = inds.shape[1]\n",
    "    word_emb = parameters['word_emb']\n",
    "    word_vec = word_emb[inds.flatten(), :].T\n",
    "    \n",
    "    assert(word_vec.shape == (word_emb.shape[1], m))\n",
    "    \n",
    "    return word_vec\n",
    "\n",
    "def linear_dense(word_vec, parameters):\n",
    "    \"\"\"\n",
    "    word_vec: numpy array. shape: (emb_size, m)\n",
    "    parameters: dict.  weights to be trained\n",
    "    \"\"\"\n",
    "    m = word_vec.shape[1]\n",
    "    W = parameters['W']\n",
    "    Z = np.dot(W, word_vec)\n",
    "    \n",
    "    assert(Z.shape == W.shape[0], m)\n",
    "    \n",
    "    return W, Z\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Z: output out of the dense layer. dim(vocab_size, m)\n",
    "    \"\"\"\n",
    "    softmax_out = np.divide(np.exp(Z), np.sum(np.exp(Z), axis = 0,\n",
    "                           keepdims = True) + 0.001)\n",
    "    \n",
    "    assert(softmax_out.shape == Z.shape)\n",
    "    \n",
    "    return softmax_out\n",
    "\n",
    "def forward_propagation(inds, parameters):\n",
    "    word_vec = ind_to_word_vecs(inds, parameters)\n",
    "    W,Z = linear_dense(word_vec, parameters)\n",
    "    softmax_out = softmax(Z)\n",
    "    \n",
    "    caches = {}\n",
    "    caches['inds'] = inds\n",
    "    caches['word_vec'] = word_vec\n",
    "    caches[\"W\"] = W\n",
    "    caches[\"Z\"] = Z\n",
    "    \n",
    "    return softmax_out, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cross entropy\n",
    "def cross_entropy(softmax_out, Y):\n",
    "    \n",
    "    m = softmax_out.shape[1]\n",
    "    cost = -(1/m) * np.sum(np.sum(Y * np.log(softmax_out + 0.001), \n",
    "                                  axis = 0, keepdims=True), axis=1)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Backward propagation\n",
    "\n",
    "def softmax_backward(Y, softmax_out):\n",
    "    \"\"\"\n",
    "    Y: labels of training data. shape: (vocab_size, m)\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    dL_dZ = softmax_out - Y\n",
    "    \n",
    "    assert(dL_dZ.shape == softmax_out.shape)\n",
    "    return dL_dZ\n",
    "\n",
    "\n",
    "def dense_backward(dL_dZ, caches):\n",
    "    \"\"\"\n",
    "    dL_dZ: shape: (vocab_size, m)\n",
    "    caches: dict. results from each steps of forward propagation\n",
    "    \"\"\"\n",
    "    W = caches['W']\n",
    "    word_vec = caches['word_vec']\n",
    "    m = word_vec.shape[1]\n",
    "    \n",
    "    dL_dW = (1 / m) * np.dot(dL_dZ, word_vec.T)\n",
    "    dL_dword_vec = np.dot(W.T, dL_dZ)\n",
    "\n",
    "    assert(W.shape == dL_dW.shape)\n",
    "    assert(word_vec.shape == dL_dword_vec.shape)\n",
    "    \n",
    "    return dL_dW, dL_dword_vec\n",
    "\n",
    "\n",
    "def backward_propagation(Y, softmax_out, caches):\n",
    "    dL_dZ = softmax_backward(Y, softmax_out)\n",
    "    dL_dW, dL_dword_vec = dense_backward(dL_dZ, caches)\n",
    "    \n",
    "    gradients = dict()\n",
    "    gradients['dL_dZ'] = dL_dZ\n",
    "    gradients['dL_dW'] = dL_dW\n",
    "    gradients['dL_dword_vec'] = dL_dword_vec\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, caches, gradients, learning_rate):\n",
    "    vocab_size, emb_size = parameters['word_emb'].shape\n",
    "    inds = caches['inds']\n",
    "    word_emb = parameters['word_emb']\n",
    "    dL_dword_vec = gradients['dL_dword_vec']\n",
    "    m = inds.shape[-1]\n",
    "    \n",
    "    word_emb[inds.flatten(), :] -= dL_dword_vec.T * learning_rate\n",
    "    \n",
    "    parameters['W'] -= learning_rate * gradients['dL_dW']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Training\n",
    "\n",
    "def skipgram_model_training(X, Y, vocab_size, emb_size, learning_rate, epochs,\n",
    "                           batch_size = 256, parameters = None, print_cost = True, \n",
    "                           plot_cost = True):\n",
    "    \"\"\"\n",
    "    X: Input word indices. shape: (1, m)\n",
    "    Y: One-hot encodeing of output word indices. shape: (vocab_size, m)\n",
    "    vocab_size: vocabulary size of your corpus or training data\n",
    "    emb_size: word embedding size. How many dimensions to represent each vocabulary\n",
    "    learning_rate: alaph in the weight update formula\n",
    "    epochs: how many epochs to train the model\n",
    "    batch_size: size of mini batch\n",
    "    parameters: pre-trained or pre-initialized parameters\n",
    "    print_cost: whether or not to print costs during the training process\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    if parameters is None:\n",
    "        parameters = initialize_parameters(vocab_size, emb_size)\n",
    "    \n",
    "    begin_time = datetime.now()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_cost = 0\n",
    "        batch_inds = list(range(0, m, batch_size))\n",
    "        np.random.shuffle(batch_inds)\n",
    "        \n",
    "        for i in batch_inds:\n",
    "            X_batch = X[:, i:i + batch_size]\n",
    "            Y_batch = Y[:, i:i + batch_size]\n",
    "            \n",
    "            softmax_out, caches = forward_propagation(X_batch, parameters)\n",
    "            gradients = backward_propagation(Y_batch, softmax_out, caches)\n",
    "            update_parameters(parameters, caches, gradients, learning_rate)\n",
    "            cost = cross_entropy(softmax_out, Y_batch)\n",
    "            epoch_cost += np.squeeze(cost)\n",
    "            \n",
    "        costs.append(epoch_cost)\n",
    "        if print_cost and epoch % (epochs // 500) == 0:\n",
    "            print(\"Cost after epoch {}:{}\".format(epoch, epoch_cost))\n",
    "        if epoch % (epochs // 100) == 0:\n",
    "            learning_rate *= 0.98\n",
    "    end_time = datetime.now()\n",
    "    print('training time: {}'.format(end_time - begin_time))\n",
    "    \n",
    "    if plot_cost:\n",
    "        plt.plot(np.arange(epochs), costs)\n",
    "        plt.xlabel('# of epochs')\n",
    "        plt.ylabel('cost')\n",
    "    \n",
    "    return parameters\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0:2.071659242449151\n",
      "Cost after epoch 10:2.0714255028192876\n",
      "Cost after epoch 20:2.071180579725863\n",
      "Cost after epoch 30:2.0709086789044338\n",
      "Cost after epoch 40:2.0705935463139995\n",
      "Cost after epoch 50:2.070217401987905\n",
      "Cost after epoch 60:2.0697693707831024\n",
      "Cost after epoch 70:2.06922309548368\n",
      "Cost after epoch 80:2.0685533506269924\n",
      "Cost after epoch 90:2.067732087344345\n",
      "Cost after epoch 100:2.0667271220888503\n",
      "Cost after epoch 110:2.065525917758759\n",
      "Cost after epoch 120:2.0640763280181296\n",
      "Cost after epoch 130:2.0623332577342426\n",
      "Cost after epoch 140:2.0602505190680414\n",
      "Cost after epoch 150:2.057780084485056\n",
      "Cost after epoch 160:2.0549307226895817\n",
      "Cost after epoch 170:2.0516278286713234\n",
      "Cost after epoch 180:2.0478334891662247\n",
      "Cost after epoch 190:2.0435288152860047\n",
      "Cost after epoch 200:2.038714692988033\n",
      "Cost after epoch 210:2.0335168319962573\n",
      "Cost after epoch 220:2.0279184904463303\n",
      "Cost after epoch 230:2.0219919193181157\n",
      "Cost after epoch 240:2.015845910887613\n",
      "Cost after epoch 250:2.0096081890335546\n",
      "Cost after epoch 260:2.003523915179433\n",
      "Cost after epoch 270:1.9976141136511332\n",
      "Cost after epoch 280:1.9919675842808422\n",
      "Cost after epoch 290:1.9866576838892582\n",
      "Cost after epoch 300:1.9817263356310468\n",
      "Cost after epoch 310:1.9772645345997153\n",
      "Cost after epoch 320:1.9731781997217945\n",
      "Cost after epoch 330:1.969434021613322\n",
      "Cost after epoch 340:1.9660005057988428\n",
      "Cost after epoch 350:1.962842890555208\n",
      "Cost after epoch 360:1.959975914216899\n",
      "Cost after epoch 370:1.957310851805359\n",
      "Cost after epoch 380:1.9548061712386817\n",
      "Cost after epoch 390:1.9524254502132914\n",
      "Cost after epoch 400:1.9501319997273845\n",
      "Cost after epoch 410:1.947930441960571\n",
      "Cost after epoch 420:1.945752539436261\n",
      "Cost after epoch 430:1.9435679346088306\n",
      "Cost after epoch 440:1.9413568531811953\n",
      "Cost after epoch 450:1.9391063471522951\n",
      "Cost after epoch 460:1.9368521369189957\n",
      "Cost after epoch 470:1.9345590352457138\n",
      "Cost after epoch 480:1.932227735375084\n",
      "Cost after epoch 490:1.929867372333141\n",
      "Cost after epoch 500:1.9274898455378324\n",
      "Cost after epoch 510:1.9251516432803961\n",
      "Cost after epoch 520:1.92282842499866\n",
      "Cost after epoch 530:1.9205291157784785\n",
      "Cost after epoch 540:1.9182670268195026\n",
      "Cost after epoch 550:1.9160547341013618\n",
      "Cost after epoch 560:1.9139420375150675\n",
      "Cost after epoch 570:1.9119025769353186\n",
      "Cost after epoch 580:1.9099407105574377\n",
      "Cost after epoch 590:1.9080639934168087\n",
      "Cost after epoch 600:1.9062787768502634\n",
      "Cost after epoch 610:1.9046198109094088\n",
      "Cost after epoch 620:1.90306087927236\n",
      "Cost after epoch 630:1.9016011346835588\n",
      "Cost after epoch 640:1.9002421729548398\n",
      "Cost after epoch 650:1.898984603030654\n",
      "Cost after epoch 660:1.8978481313533648\n",
      "Cost after epoch 670:1.896810102618385\n",
      "Cost after epoch 680:1.895866357439779\n",
      "Cost after epoch 690:1.8950145847929516\n",
      "Cost after epoch 700:1.894252001455041\n",
      "Cost after epoch 710:1.893586960249003\n",
      "Cost after epoch 720:1.8930027630897304\n",
      "Cost after epoch 730:1.8924945691741746\n",
      "Cost after epoch 740:1.8920588839058943\n",
      "Cost after epoch 750:1.8916921729337919\n",
      "Cost after epoch 760:1.8913958289510846\n",
      "Cost after epoch 770:1.8911597287703035\n",
      "Cost after epoch 780:1.89097998370265\n",
      "Cost after epoch 790:1.890853456162367\n",
      "Cost after epoch 800:1.8907770941322342\n",
      "Cost after epoch 810:1.8907480994295773\n",
      "Cost after epoch 820:1.8907618289911055\n",
      "Cost after epoch 830:1.8908155337122865\n",
      "Cost after epoch 840:1.8909066412220696\n",
      "Cost after epoch 850:1.8910326399167516\n",
      "Cost after epoch 860:1.8911879799217957\n",
      "Cost after epoch 870:1.8913719159489246\n",
      "Cost after epoch 880:1.8915824992837327\n",
      "Cost after epoch 890:1.891817536393164\n",
      "Cost after epoch 900:1.8920748683710815\n",
      "Cost after epoch 910:1.8923472326287025\n",
      "Cost after epoch 920:1.8926364634398498\n",
      "Cost after epoch 930:1.8929411550346147\n",
      "Cost after epoch 940:1.8932594037210493\n",
      "Cost after epoch 950:1.8935893435312505\n",
      "Cost after epoch 960:1.8939229735792225\n",
      "Cost after epoch 970:1.8942637333849786\n",
      "Cost after epoch 980:1.8946106591854273\n",
      "Cost after epoch 990:1.8949621689021583\n",
      "Cost after epoch 1000:1.895316734600567\n",
      "Cost after epoch 1010:1.895666482380248\n",
      "Cost after epoch 1020:1.8960157173447172\n",
      "Cost after epoch 1030:1.8963638827093203\n",
      "Cost after epoch 1040:1.8967097763115508\n",
      "Cost after epoch 1050:1.8970522630987614\n",
      "Cost after epoch 1060:1.8973842475904423\n",
      "Cost after epoch 1070:1.8977103205721697\n",
      "Cost after epoch 1080:1.8980302911468516\n",
      "Cost after epoch 1090:1.898343357510036\n",
      "Cost after epoch 1100:1.8986487872432398\n",
      "Cost after epoch 1110:1.8989406529215234\n",
      "Cost after epoch 1120:1.899223387229678\n",
      "Cost after epoch 1130:1.899497094422934\n",
      "Cost after epoch 1140:1.8997613397150734\n",
      "Cost after epoch 1150:1.900015749885103\n",
      "Cost after epoch 1160:1.90025571220823\n",
      "Cost after epoch 1170:1.900485191053056\n",
      "Cost after epoch 1180:1.9007044940858726\n",
      "Cost after epoch 1190:1.9009134811635022\n",
      "Cost after epoch 1200:1.9011120600592575\n",
      "Cost after epoch 1210:1.9012968960635055\n",
      "Cost after epoch 1220:1.9014713036232835\n",
      "Cost after epoch 1230:1.9016357007138516\n",
      "Cost after epoch 1240:1.901790154790596\n",
      "Cost after epoch 1250:1.9019347662814685\n",
      "Cost after epoch 1260:1.9020673288455208\n",
      "Cost after epoch 1270:1.9021904377229726\n",
      "Cost after epoch 1280:1.902304543141879\n",
      "Cost after epoch 1290:1.9024098386369064\n",
      "Cost after epoch 1300:1.9025065375505894\n",
      "Cost after epoch 1310:1.9025933598751055\n",
      "Cost after epoch 1320:1.902672198827326\n",
      "Cost after epoch 1330:1.9027434816953654\n",
      "Cost after epoch 1340:1.9028074631146619\n",
      "Cost after epoch 1350:1.9028644074797234\n",
      "Cost after epoch 1360:1.902913749219852\n",
      "Cost after epoch 1370:1.9029567539901084\n",
      "Cost after epoch 1380:1.9029937938494814\n",
      "Cost after epoch 1390:1.9030251388533872\n",
      "Cost after epoch 1400:1.9030510619494612\n",
      "Cost after epoch 1410:1.903071515204803\n",
      "Cost after epoch 1420:1.903087241769343\n",
      "Cost after epoch 1430:1.903098544596498\n",
      "Cost after epoch 1440:1.9031056804870286\n",
      "Cost after epoch 1450:1.90310890486759\n",
      "Cost after epoch 1460:1.903108517548139\n",
      "Cost after epoch 1470:1.9031048539909092\n",
      "Cost after epoch 1480:1.903098146679313\n",
      "Cost after epoch 1490:1.903088623228588\n",
      "Cost after epoch 1500:1.9030765074637226\n",
      "Cost after epoch 1510:1.9030623075669846\n",
      "Cost after epoch 1520:1.9030460617176383\n",
      "Cost after epoch 1530:1.9030279377467372\n",
      "Cost after epoch 1540:1.9030081266636036\n",
      "Cost after epoch 1550:1.9029868144665412\n",
      "Cost after epoch 1560:1.9029646081839717\n",
      "Cost after epoch 1570:1.902941346384415\n",
      "Cost after epoch 1580:1.9029171413198414\n",
      "Cost after epoch 1590:1.902892145191876\n",
      "Cost after epoch 1600:1.9028665046998792\n",
      "Cost after epoch 1610:1.9028408435118491\n",
      "Cost after epoch 1620:1.90281488159751\n",
      "Cost after epoch 1630:1.9027886855011231\n",
      "Cost after epoch 1640:1.9027623694553049\n",
      "Cost after epoch 1650:1.9027360421445252\n",
      "Cost after epoch 1660:1.902710285888671\n",
      "Cost after epoch 1670:1.9026847654082506\n",
      "Cost after epoch 1680:1.902659510915421\n",
      "Cost after epoch 1690:1.902634601286696\n",
      "Cost after epoch 1700:1.902610110097432\n",
      "Cost after epoch 1710:1.9025865413725835\n",
      "Cost after epoch 1720:1.9025635510269416\n",
      "Cost after epoch 1730:1.9025411412895512\n",
      "Cost after epoch 1740:1.9025193593972893\n",
      "Cost after epoch 1750:1.9024982477487162\n",
      "Cost after epoch 1760:1.902478213014208\n",
      "Cost after epoch 1770:1.9024589335512239\n",
      "Cost after epoch 1780:1.9024403908111918\n",
      "Cost after epoch 1790:1.9024226048499748\n",
      "Cost after epoch 1800:1.9024055915179947\n",
      "Cost after epoch 1810:1.9023896557954252\n",
      "Cost after epoch 1820:1.902374516704546\n",
      "Cost after epoch 1830:1.902360141035563\n",
      "Cost after epoch 1840:1.9023465266144566\n",
      "Cost after epoch 1850:1.9023336678233203\n",
      "Cost after epoch 1860:1.9023217751449248\n",
      "Cost after epoch 1870:1.9023106158048277\n",
      "Cost after epoch 1880:1.9023001470494516\n",
      "Cost after epoch 1890:1.902290349678125\n",
      "Cost after epoch 1900:1.9022812018888628\n",
      "Cost after epoch 1910:1.9022728352036178\n",
      "Cost after epoch 1920:1.9022650642626568\n",
      "Cost after epoch 1930:1.9022578409596853\n",
      "Cost after epoch 1940:1.902251134263074\n",
      "Cost after epoch 1950:1.902244911409781\n",
      "Cost after epoch 1960:1.902239245773783\n",
      "Cost after epoch 1970:1.9022339925674123\n",
      "Cost after epoch 1980:1.9022291016478547\n",
      "Cost after epoch 1990:1.9022245350772247\n",
      "Cost after epoch 2000:1.9022202540295525\n",
      "Cost after epoch 2010:1.9022162970059517\n",
      "Cost after epoch 2020:1.9022125490108786\n",
      "Cost after epoch 2030:1.9022089603541268\n",
      "Cost after epoch 2040:1.9022054906096324\n",
      "Cost after epoch 2050:1.9022020992326294\n",
      "Cost after epoch 2060:1.9021988131574645\n",
      "Cost after epoch 2070:1.9021955343621282\n",
      "Cost after epoch 2080:1.9021922153339033\n",
      "Cost after epoch 2090:1.9021888168905585\n",
      "Cost after epoch 2100:1.9021853003908324\n",
      "Cost after epoch 2110:1.902181702584456\n",
      "Cost after epoch 2120:1.9021779291937595\n",
      "Cost after epoch 2130:1.9021739359320091\n",
      "Cost after epoch 2140:1.9021696877993617\n",
      "Cost after epoch 2150:1.9021651508649884\n",
      "Cost after epoch 2160:1.9021603897185544\n",
      "Cost after epoch 2170:1.9021553015059633\n",
      "Cost after epoch 2180:1.9021498456511698\n",
      "Cost after epoch 2190:1.9021439934481263\n",
      "Cost after epoch 2200:1.9021377176479373\n",
      "Cost after epoch 2210:1.9021311244311063\n",
      "Cost after epoch 2220:1.9021240919455487\n",
      "Cost after epoch 2230:1.90211658337611\n",
      "Cost after epoch 2240:1.9021085776256421\n",
      "Cost after epoch 2250:1.9021000553057437\n",
      "Cost after epoch 2260:1.9020911732233674\n",
      "Cost after epoch 2270:1.9020817826706713\n",
      "Cost after epoch 2280:1.9020718503435818\n",
      "Cost after epoch 2290:1.9020613633527035\n",
      "Cost after epoch 2300:1.9020503106446043\n",
      "Cost after epoch 2310:1.9020389039357346\n",
      "Cost after epoch 2320:1.9020269627289559\n",
      "Cost after epoch 2330:1.902014456809702\n",
      "Cost after epoch 2340:1.9020013814910712\n",
      "Cost after epoch 2350:1.9019877339429163\n",
      "Cost after epoch 2360:1.9019737806045456\n",
      "Cost after epoch 2370:1.9019593072743284\n",
      "Cost after epoch 2380:1.9019442863468414\n",
      "Cost after epoch 2390:1.9019287208564621\n",
      "Cost after epoch 2400:1.9019126156296644\n",
      "Cost after epoch 2410:1.9018962876875332\n",
      "Cost after epoch 2420:1.90187949010295\n",
      "Cost after epoch 2430:1.9018621974322718\n",
      "Cost after epoch 2440:1.901844419604795\n",
      "Cost after epoch 2450:1.9018261682135083\n",
      "Cost after epoch 2460:1.901807803429747\n",
      "Cost after epoch 2470:1.9017890491843854\n",
      "Cost after epoch 2480:1.9017698818490898\n",
      "Cost after epoch 2490:1.9017503171998427\n",
      "Cost after epoch 2500:1.9017303725048902\n",
      "Cost after epoch 2510:1.9017104411024779\n",
      "Cost after epoch 2520:1.9016902235664308\n",
      "Cost after epoch 2530:1.9016696978819945\n",
      "Cost after epoch 2540:1.9016488845105624\n",
      "Cost after epoch 2550:1.9016278052098208\n",
      "Cost after epoch 2560:1.9016068747013968\n",
      "Cost after epoch 2570:1.901585777720421\n",
      "Cost after epoch 2580:1.9015644938304306\n",
      "Cost after epoch 2590:1.9015430469941825\n",
      "Cost after epoch 2600:1.9015214622657524\n",
      "Cost after epoch 2610:1.9015001628831483\n",
      "Cost after epoch 2620:1.901478826503066\n",
      "Cost after epoch 2630:1.9014574344011317\n",
      "Cost after epoch 2640:1.9014360129047487\n",
      "Cost after epoch 2650:1.9014145892309307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 2660:1.901393581831173\n",
      "Cost after epoch 2670:1.9013726708047047\n",
      "Cost after epoch 2680:1.901351839429625\n",
      "Cost after epoch 2690:1.9013311153587853\n",
      "Cost after epoch 2700:1.9013105269452075\n",
      "Cost after epoch 2710:1.9012904746720087\n",
      "Cost after epoch 2720:1.901270650917086\n",
      "Cost after epoch 2730:1.9012510413844672\n",
      "Cost after epoch 2740:1.9012316741427446\n",
      "Cost after epoch 2750:1.9012125777894873\n",
      "Cost after epoch 2760:1.901194122234183\n",
      "Cost after epoch 2770:1.9011760218921856\n",
      "Cost after epoch 2780:1.9011582654195038\n",
      "Cost after epoch 2790:1.90114088053656\n",
      "Cost after epoch 2800:1.901123895343198\n",
      "Cost after epoch 2810:1.9011076374991283\n",
      "Cost after epoch 2820:1.9010918528092553\n",
      "Cost after epoch 2830:1.901076533476388\n",
      "Cost after epoch 2840:1.901061706258406\n",
      "Cost after epoch 2850:1.901047398165944\n",
      "Cost after epoch 2860:1.9010338841161516\n",
      "Cost after epoch 2870:1.9010209499867237\n",
      "Cost after epoch 2880:1.901008592156762\n",
      "Cost after epoch 2890:1.9009968359510534\n",
      "Cost after epoch 2900:1.900985706843011\n",
      "Cost after epoch 2910:1.900975417863224\n",
      "Cost after epoch 2920:1.900965802780822\n",
      "Cost after epoch 2930:1.900956862783378\n",
      "Cost after epoch 2940:1.9009486214236597\n",
      "Cost after epoch 2950:1.9009411023200034\n",
      "Cost after epoch 2960:1.9009344489099054\n",
      "Cost after epoch 2970:1.9009285496742971\n",
      "Cost after epoch 2980:1.9009234112158755\n",
      "Cost after epoch 2990:1.9009190550920176\n",
      "Cost after epoch 3000:1.9009155028615032\n",
      "Cost after epoch 3010:1.9009128221248242\n",
      "Cost after epoch 3020:1.9009109618184812\n",
      "Cost after epoch 3030:1.9009099345180314\n",
      "Cost after epoch 3040:1.9009097596597893\n",
      "Cost after epoch 3050:1.9009104566336619\n",
      "Cost after epoch 3060:1.9009120123966967\n",
      "Cost after epoch 3070:1.9009144409771133\n",
      "Cost after epoch 3080:1.9009177614140162\n",
      "Cost after epoch 3090:1.9009219909766766\n",
      "Cost after epoch 3100:1.9009271468538809\n",
      "Cost after epoch 3110:1.9009331319270921\n",
      "Cost after epoch 3120:1.9009400288525806\n",
      "Cost after epoch 3130:1.9009478635437491\n",
      "Cost after epoch 3140:1.9009566511195075\n",
      "Cost after epoch 3150:1.900966406595399\n",
      "Cost after epoch 3160:1.9009769466292918\n",
      "Cost after epoch 3170:1.900988424980218\n",
      "Cost after epoch 3180:1.9010008747587352\n",
      "Cost after epoch 3190:1.9010143089973859\n",
      "Cost after epoch 3200:1.9010287406113522\n",
      "Cost after epoch 3210:1.901043899063157\n",
      "Cost after epoch 3220:1.9010600106885691\n",
      "Cost after epoch 3230:1.9010771160266302\n",
      "Cost after epoch 3240:1.9010952261215528\n",
      "Cost after epoch 3250:1.9011143518930433\n",
      "Cost after epoch 3260:1.901134135694901\n",
      "Cost after epoch 3270:1.9011548769895257\n",
      "Cost after epoch 3280:1.901176623884287\n",
      "Cost after epoch 3290:1.9011993855560412\n",
      "Cost after epoch 3300:1.9012231710551346\n",
      "Cost after epoch 3310:1.9012475366497563\n",
      "Cost after epoch 3320:1.9012728546413415\n",
      "Cost after epoch 3330:1.90129918075689\n",
      "Cost after epoch 3340:1.9013265224408555\n",
      "Cost after epoch 3350:1.9013548870128898\n",
      "Cost after epoch 3360:1.9013837464939074\n",
      "Cost after epoch 3370:1.9014135449909912\n",
      "Cost after epoch 3380:1.9014443458187174\n",
      "Cost after epoch 3390:1.901476154831026\n",
      "Cost after epoch 3400:1.901508977761349\n",
      "Cost after epoch 3410:1.90154220491954\n",
      "Cost after epoch 3420:1.9015763505321601\n",
      "Cost after epoch 3430:1.9016114853926995\n",
      "Cost after epoch 3440:1.901647613908078\n",
      "Cost after epoch 3450:1.9016847403706985\n",
      "Cost after epoch 3460:1.9017221765023447\n",
      "Cost after epoch 3470:1.9017605044015076\n",
      "Cost after epoch 3480:1.9017998021630909\n",
      "Cost after epoch 3490:1.9018400728881806\n",
      "Cost after epoch 3500:1.9018813195703737\n",
      "Cost after epoch 3510:1.9019227789497108\n",
      "Cost after epoch 3520:1.901965098275424\n",
      "Cost after epoch 3530:1.9020083627057738\n",
      "Cost after epoch 3540:1.902052574172174\n",
      "Cost after epoch 3550:1.9020977345060994\n",
      "Cost after epoch 3560:1.9021430094605705\n",
      "Cost after epoch 3570:1.90218910832331\n",
      "Cost after epoch 3580:1.9022361230257963\n",
      "Cost after epoch 3590:1.9022840544590849\n",
      "Cost after epoch 3600:1.9023329034220244\n",
      "Cost after epoch 3610:1.9023817689775782\n",
      "Cost after epoch 3620:1.9024314190251956\n",
      "Cost after epoch 3630:1.90248195193569\n",
      "Cost after epoch 3640:1.902533367681136\n",
      "Cost after epoch 3650:1.9025856661490697\n",
      "Cost after epoch 3660:1.9026378842342404\n",
      "Cost after epoch 3670:1.9026908447765627\n",
      "Cost after epoch 3680:1.9027446522165434\n",
      "Cost after epoch 3690:1.9027993057201742\n",
      "Cost after epoch 3700:1.902854804376335\n",
      "Cost after epoch 3710:1.9029101275894074\n",
      "Cost after epoch 3720:1.9029661492873233\n",
      "Cost after epoch 3730:1.9030229795824196\n",
      "Cost after epoch 3740:1.9030806169388355\n",
      "Cost after epoch 3750:1.903139059750689\n",
      "Cost after epoch 3760:1.9031972347050448\n",
      "Cost after epoch 3770:1.9032560628408988\n",
      "Cost after epoch 3780:1.9033156595232683\n",
      "Cost after epoch 3790:1.9033760226101093\n",
      "Cost after epoch 3800:1.9034371498960576\n",
      "Cost after epoch 3810:1.9034979201658546\n",
      "Cost after epoch 3820:1.9035592975183624\n",
      "Cost after epoch 3830:1.9036214021371733\n",
      "Cost after epoch 3840:1.9036842313614484\n",
      "Cost after epoch 3850:1.9037477824733129\n",
      "Cost after epoch 3860:1.9038108911658291\n",
      "Cost after epoch 3870:1.9038745605164862\n",
      "Cost after epoch 3880:1.9039389150840533\n",
      "Cost after epoch 3890:1.904003951768403\n",
      "Cost after epoch 3900:1.9040696674182314\n",
      "Cost after epoch 3910:1.9041348594013157\n",
      "Cost after epoch 3920:1.9042005657009853\n",
      "Cost after epoch 3930:1.9042669148035516\n",
      "Cost after epoch 3940:1.904333903241676\n",
      "Cost after epoch 3950:1.904401527502285\n",
      "Cost after epoch 3960:1.9044685513159123\n",
      "Cost after epoch 3970:1.9045360435406544\n",
      "Cost after epoch 3980:1.904604136142916\n",
      "Cost after epoch 3990:1.9046728253532845\n",
      "Cost after epoch 4000:1.9047421073616533\n",
      "Cost after epoch 4010:1.9048107168421478\n",
      "Cost after epoch 4020:1.9048797495667158\n",
      "Cost after epoch 4030:1.9049493405384428\n",
      "Cost after epoch 4040:1.9050194857445262\n",
      "Cost after epoch 4050:1.9050901811361316\n",
      "Cost after epoch 4060:1.90516013678049\n",
      "Cost after epoch 4070:1.9052304714967105\n",
      "Cost after epoch 4080:1.9053013228884785\n",
      "Cost after epoch 4090:1.905372686752318\n",
      "Cost after epoch 4100:1.9054445588530267\n",
      "Cost after epoch 4110:1.9055156289492514\n",
      "Cost after epoch 4120:1.9055870351548994\n",
      "Cost after epoch 4130:1.9056589172483192\n",
      "Cost after epoch 4140:1.905731270882569\n",
      "Cost after epoch 4150:1.905804091682951\n",
      "Cost after epoch 4160:1.9058760532305474\n",
      "Cost after epoch 4170:1.9059483093125777\n",
      "Cost after epoch 4180:1.9060210014686036\n",
      "Cost after epoch 4190:1.9060941252503536\n",
      "Cost after epoch 4200:1.9061676761854525\n",
      "Cost after epoch 4210:1.9062403156284409\n",
      "Cost after epoch 4220:1.9063132095625939\n",
      "Cost after epoch 4230:1.90638650088966\n",
      "Cost after epoch 4240:1.906460185097505\n",
      "Cost after epoch 4250:1.906534257653253\n",
      "Cost after epoch 4260:1.90660737144623\n",
      "Cost after epoch 4270:1.9066807013332343\n",
      "Cost after epoch 4280:1.906754391195163\n",
      "Cost after epoch 4290:1.9068284364892016\n",
      "Cost after epoch 4300:1.9069028326548838\n",
      "Cost after epoch 4310:1.9069762276809143\n",
      "Cost after epoch 4320:1.9070498021377738\n",
      "Cost after epoch 4330:1.9071237005196913\n",
      "Cost after epoch 4340:1.9071979182824228\n",
      "Cost after epoch 4350:1.9072724508669023\n",
      "Cost after epoch 4360:1.9073459447243908\n",
      "Cost after epoch 4370:1.9074195831476233\n",
      "Cost after epoch 4380:1.9074935108964963\n",
      "Cost after epoch 4390:1.9075677234509834\n",
      "Cost after epoch 4400:1.9076422162788276\n",
      "Cost after epoch 4410:1.9077156374529796\n",
      "Cost after epoch 4420:1.907789170169151\n",
      "Cost after epoch 4430:1.9078629591240743\n",
      "Cost after epoch 4440:1.9079369998442854\n",
      "Cost after epoch 4450:1.9080112878464561\n",
      "Cost after epoch 4460:1.9080844757795314\n",
      "Cost after epoch 4470:1.908157744097053\n",
      "Cost after epoch 4480:1.908231237123032\n",
      "Cost after epoch 4490:1.908304950449898\n",
      "Cost after epoch 4500:1.908378879662369\n",
      "Cost after epoch 4510:1.908451684735617\n",
      "Cost after epoch 4520:1.9085245409104792\n",
      "Cost after epoch 4530:1.9085975918482077\n",
      "Cost after epoch 4540:1.9086708332236975\n",
      "Cost after epoch 4550:1.9087442607060894\n",
      "Cost after epoch 4560:1.9088165441450933\n",
      "Cost after epoch 4570:1.9088888512720577\n",
      "Cost after epoch 4580:1.908961324815018\n",
      "Cost after epoch 4590:1.9090339605453963\n",
      "Cost after epoch 4600:1.909106754230635\n",
      "Cost after epoch 4610:1.9091783879446884\n",
      "Cost after epoch 4620:1.9092500197843767\n",
      "Cost after epoch 4630:1.9093217912935534\n",
      "Cost after epoch 4640:1.909393698351949\n",
      "Cost after epoch 4650:1.9094657368369112\n",
      "Cost after epoch 4660:1.9095366032020606\n",
      "Cost after epoch 4670:1.9096074439534072\n",
      "Cost after epoch 4680:1.9096783992189332\n",
      "Cost after epoch 4690:1.9097494649963862\n",
      "Cost after epoch 4700:1.9098206372825615\n",
      "Cost after epoch 4710:1.9098906288770465\n",
      "Cost after epoch 4720:1.9099605729036913\n",
      "Cost after epoch 4730:1.9100306078618607\n",
      "Cost after epoch 4740:1.9101007298751491\n",
      "Cost after epoch 4750:1.9101709350674827\n",
      "Cost after epoch 4760:1.9102399543674546\n",
      "Cost after epoch 4770:1.9103089058858211\n",
      "Cost after epoch 4780:1.9103779262990888\n",
      "Cost after epoch 4790:1.9104470118628427\n",
      "Cost after epoch 4800:1.9105161588341362\n",
      "Cost after epoch 4810:1.9105841178767915\n",
      "Cost after epoch 4820:1.910651990612836\n",
      "Cost after epoch 4830:1.9107199117196847\n",
      "Cost after epoch 4840:1.910787877589536\n",
      "Cost after epoch 4850:1.9108558846170576\n",
      "Cost after epoch 4860:1.9109227046375905\n",
      "Cost after epoch 4870:1.9109894214585317\n",
      "Cost after epoch 4880:1.9110561675993956\n",
      "Cost after epoch 4890:1.9111229395922518\n",
      "Cost after epoch 4900:1.9111897339725195\n",
      "Cost after epoch 4910:1.911255345020671\n",
      "Cost after epoch 4920:1.9113208375473398\n",
      "Cost after epoch 4930:1.9113863417721317\n",
      "Cost after epoch 4940:1.9114518543690313\n",
      "Cost after epoch 4950:1.9115173720161391\n",
      "Cost after epoch 4960:1.9115817125574934\n",
      "Cost after epoch 4970:1.9116459207623755\n",
      "Cost after epoch 4980:1.9117101244245913\n",
      "Cost after epoch 4990:1.9117743203610094\n",
      "training time: 0:00:01.153938\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "paras = skipgram_model_training(X, Y_one_hot, \n",
    "                               vocab_size, 50, 0.05, 5000,\n",
    "                               batch_size=128,\n",
    "                               parameters = None, \n",
    "                               print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word_emb': array([[ 2.41174231e-02, -1.46284402e-02,  1.12994349e-01,\n",
       "          3.73440507e-03, -5.78437821e-01, -5.37531484e-01,\n",
       "          6.08771225e-01,  9.93836777e-01,  2.88522772e+00,\n",
       "         -2.01546164e+00, -2.99384970e-01,  4.61382559e-01,\n",
       "         -2.48380564e-01,  1.04415380e+00,  1.63048631e+00,\n",
       "          2.75511011e-01,  3.29711581e-01, -1.13138117e+00,\n",
       "          2.09192883e+00, -1.27995976e+00,  1.69790734e+00,\n",
       "         -9.92867444e-01,  3.04523778e-01,  5.41178792e-01,\n",
       "         -1.58450586e-01, -2.44165285e-01, -3.31673554e-01,\n",
       "          2.44655737e+00, -1.68837879e-01,  2.36518916e+00,\n",
       "         -2.57988941e-01,  8.50225120e-01, -6.99702994e-01,\n",
       "          8.50493147e-02, -1.75359313e+00,  9.09151375e-01,\n",
       "         -2.04974154e+00, -4.11253571e-01,  5.33616024e-01,\n",
       "         -1.49670967e+00, -7.39744318e-01, -7.23496375e-01,\n",
       "          2.16070893e+00, -1.39366352e+00, -2.25593407e-01,\n",
       "         -1.90422168e+00, -2.21015646e-01,  2.21737590e-01,\n",
       "         -1.13734965e+00, -2.10371677e+00],\n",
       "        [ 3.38277771e-02, -1.48676808e-02,  1.03678809e-01,\n",
       "          1.11161375e-02, -5.78830164e-01, -5.38646843e-01,\n",
       "          6.10843194e-01,  1.00609995e+00,  2.88736544e+00,\n",
       "         -2.02878248e+00, -2.91862095e-01,  4.72895656e-01,\n",
       "         -2.32652296e-01,  1.05945308e+00,  1.63261988e+00,\n",
       "          2.69335387e-01,  3.32046678e-01, -1.12938380e+00,\n",
       "          2.09309690e+00, -1.29602717e+00,  1.70959761e+00,\n",
       "         -9.92157892e-01,  2.87633038e-01,  5.56009713e-01,\n",
       "         -1.50686134e-01, -2.58123705e-01, -3.31819785e-01,\n",
       "          2.45754335e+00, -1.59081329e-01,  2.38583584e+00,\n",
       "         -2.56674783e-01,  8.52895430e-01, -7.01864483e-01,\n",
       "          8.50739537e-02, -1.77843205e+00,  9.01975254e-01,\n",
       "         -2.04513637e+00, -4.21406821e-01,  5.27776202e-01,\n",
       "         -1.50488565e+00, -7.41632340e-01, -7.31589360e-01,\n",
       "          2.15456892e+00, -1.39004402e+00, -2.29566374e-01,\n",
       "         -1.91679008e+00, -2.25100394e-01,  2.21821112e-01,\n",
       "         -1.13736143e+00, -2.08451620e+00],\n",
       "        [ 2.31794696e-01,  1.26200387e-01,  1.78418707e+00,\n",
       "         -7.22761569e-01,  2.24487492e+00, -2.23310808e+00,\n",
       "          1.48627510e+00, -1.28527637e+00, -2.03074369e-01,\n",
       "          1.63507649e+00, -1.37060156e+00,  3.69316144e-01,\n",
       "         -2.14695058e+00, -4.56651997e-01,  1.17018147e+00,\n",
       "          3.16356470e+00, -1.66106811e+00, -2.03176935e+00,\n",
       "          1.68266459e+00, -8.65897645e-01, -1.85198447e+00,\n",
       "         -8.94951664e-01, -8.06244826e-01, -9.46639566e-01,\n",
       "         -1.30002817e+00,  1.36660869e+00,  1.18763693e+00,\n",
       "         -4.56016187e-01, -1.71426571e+00, -5.11975551e-01,\n",
       "         -1.91677699e+00, -3.39503657e-01, -5.74399044e-01,\n",
       "         -1.66166443e-01,  2.92456270e-01, -2.61278662e+00,\n",
       "         -2.11166910e+00,  8.58300679e-01,  8.91370149e-02,\n",
       "          1.10938449e+00, -2.38996249e+00, -3.91416625e-01,\n",
       "         -7.23215986e-01,  5.94949147e-01,  2.73100937e+00,\n",
       "          2.69663267e-01,  9.87674392e-01,  5.74943811e-01,\n",
       "          2.25293270e-01, -3.75253565e+00],\n",
       "        [ 5.96840519e-01,  3.44825290e-02, -9.71046818e-01,\n",
       "          9.88195301e-01,  1.35589241e-01,  1.10045274e+00,\n",
       "         -5.05151169e-01,  1.15959502e-01, -2.87677066e+00,\n",
       "          9.13067316e-01,  6.26814514e-01, -3.14710101e-01,\n",
       "          2.00478767e+00,  8.27987680e-02, -2.06343427e+00,\n",
       "         -1.56255244e+00,  3.24155183e-01,  1.30800117e+00,\n",
       "         -2.73508419e+00,  1.47977930e+00, -1.48912630e+00,\n",
       "          1.35376335e+00, -7.70792255e-01, -3.57364284e-01,\n",
       "          8.35650505e-01, -4.55617780e-01,  9.29576240e-01,\n",
       "         -2.22039531e+00,  1.18674909e+00, -8.63265062e-01,\n",
       "          7.40545820e-01, -1.30533939e-01,  7.75251713e-01,\n",
       "         -3.92442251e-01, -2.65541007e-02, -6.99590151e-01,\n",
       "          3.93544589e+00, -5.67376364e-01, -1.60736364e+00,\n",
       "          1.26749960e+00,  2.20752203e+00,  5.20305322e-01,\n",
       "         -2.75162387e+00,  1.23321434e+00, -1.00192299e+00,\n",
       "          1.36521636e+00, -1.31086856e+00, -7.93586270e-01,\n",
       "          9.88223027e-01,  4.55009295e+00],\n",
       "        [-6.43835762e-01, -3.30912053e-01,  1.85865350e-01,\n",
       "         -3.93360043e-01,  5.38330739e-01,  4.44218151e-01,\n",
       "         -8.76107217e-01, -1.07199759e+00, -2.54168910e+00,\n",
       "          1.85730562e+00,  4.18636871e-01, -8.49493199e-01,\n",
       "         -4.94954566e-01, -1.11285692e+00, -1.50074488e+00,\n",
       "         -5.21928789e-01, -6.17717980e-01,  1.35454378e+00,\n",
       "         -1.85889215e+00,  1.14730771e+00, -1.17153367e+00,\n",
       "          8.04424170e-01,  7.90980495e-02, -1.99135210e-01,\n",
       "          7.03858530e-02,  1.78474178e-01, -3.15047504e-01,\n",
       "         -1.88766940e+00, -3.76794644e-02, -2.87695290e+00,\n",
       "          4.45215711e-01, -1.46572575e+00,  5.77974848e-01,\n",
       "          1.31554041e-01,  2.25321330e+00, -6.25233926e-01,\n",
       "          1.29570815e+00,  7.02551688e-01,  1.82300807e-01,\n",
       "          1.08672274e+00,  1.03197886e-01,  4.87298253e-01,\n",
       "         -1.43449277e+00,  1.16235865e+00,  7.78972362e-01,\n",
       "          2.11455090e+00,  1.22366043e+00, -2.92747788e-02,\n",
       "          9.09549763e-01,  1.43032896e+00],\n",
       "        [ 2.29013150e-01,  1.14065105e-01,  1.80820592e+00,\n",
       "         -6.90631691e-01,  2.23845928e+00, -2.24393748e+00,\n",
       "          1.50021168e+00, -1.28802217e+00, -1.98895866e-01,\n",
       "          1.63049259e+00, -1.36793693e+00,  3.74469999e-01,\n",
       "         -2.14510357e+00, -4.44164300e-01,  1.17533189e+00,\n",
       "          3.16945552e+00, -1.66691444e+00, -2.02550223e+00,\n",
       "          1.69007172e+00, -8.47299882e-01, -1.87895090e+00,\n",
       "         -8.93828533e-01, -7.82182169e-01, -9.51419478e-01,\n",
       "         -1.28955963e+00,  1.34059331e+00,  1.20006939e+00,\n",
       "         -4.62490278e-01, -1.71679720e+00, -4.99824472e-01,\n",
       "         -1.89710738e+00, -3.42675011e-01, -5.94592541e-01,\n",
       "         -1.82604611e-01,  2.95873589e-01, -2.62809534e+00,\n",
       "         -2.11436621e+00,  8.63934238e-01,  6.55374017e-02,\n",
       "          1.11440533e+00, -2.38170704e+00, -4.08825016e-01,\n",
       "         -7.29130220e-01,  5.84641634e-01,  2.73522099e+00,\n",
       "          2.70748692e-01,  9.88941150e-01,  5.84075329e-01,\n",
       "          2.49717329e-01, -3.74725374e+00],\n",
       "        [ 4.45582486e-02, -7.84120612e-03,  9.84876141e-02,\n",
       "          4.65648968e-03, -5.76659538e-01, -5.40373518e-01,\n",
       "          6.15853330e-01,  9.97856891e-01,  2.88381634e+00,\n",
       "         -2.01792250e+00, -2.98118998e-01,  4.65525916e-01,\n",
       "         -2.31026064e-01,  1.02971764e+00,  1.64066452e+00,\n",
       "          2.77196382e-01,  3.34968723e-01, -1.13915166e+00,\n",
       "          2.08950613e+00, -1.30601002e+00,  1.69714543e+00,\n",
       "         -1.00597828e+00,  2.80457492e-01,  5.38637859e-01,\n",
       "         -1.57939558e-01, -2.34994087e-01, -3.23320976e-01,\n",
       "          2.43846003e+00, -1.59679675e-01,  2.38369895e+00,\n",
       "         -2.74052615e-01,  8.66469343e-01, -6.89378493e-01,\n",
       "          8.21961479e-02, -1.77577779e+00,  9.01687953e-01,\n",
       "         -2.05344356e+00, -4.34748266e-01,  5.28075387e-01,\n",
       "         -1.50652732e+00, -7.44756211e-01, -7.25596674e-01,\n",
       "          2.15379204e+00, -1.38546510e+00, -2.41545779e-01,\n",
       "         -1.91554043e+00, -2.33843383e-01,  2.19187023e-01,\n",
       "         -1.12769522e+00, -2.09147361e+00],\n",
       "        [-1.20085717e-01, -1.49889573e-01, -1.90225802e+00,\n",
       "          9.15470371e-01, -2.62671296e+00,  2.13061778e+00,\n",
       "         -1.22119810e+00,  1.96439058e+00,  1.49962985e+00,\n",
       "         -2.76929100e+00,  1.38526043e+00, -1.40298442e-01,\n",
       "          2.38007124e+00,  1.15200652e+00, -5.31317824e-01,\n",
       "         -3.36974563e+00,  1.94380694e+00,  1.64516023e+00,\n",
       "         -8.65440655e-01,  3.07051796e-01,  2.72805498e+00,\n",
       "          5.31981036e-01,  8.63420198e-01,  1.32417394e+00,\n",
       "          1.38845612e+00, -1.69689539e+00, -1.31964067e+00,\n",
       "          1.65506893e+00,  1.83959458e+00,  1.83787760e+00,\n",
       "          1.95763473e+00,  8.16954423e-01,  2.54918099e-01,\n",
       "          1.56515528e-01, -1.39499196e+00,  3.06454166e+00,\n",
       "          1.53128564e+00, -1.21842381e+00, -2.46433513e-02,\n",
       "         -1.86801387e+00,  2.28576763e+00, -5.47228285e-03,\n",
       "          1.60815071e+00, -1.24271523e+00, -3.05743894e+00,\n",
       "         -1.21938364e+00, -1.27813094e+00, -5.60600571e-01,\n",
       "         -7.54405412e-01,  3.32564868e+00]]),\n",
       " 'W': array([[ 0.0862965 , -0.06189067,  0.07995385, -0.04014934,  0.1608021 ,\n",
       "          0.05950983,  0.23052485,  0.16494183, -0.05140154, -0.05932296,\n",
       "          0.06460783, -0.40991127,  0.08860857, -0.10782563,  0.04801946,\n",
       "         -0.42486736,  0.03845818, -0.00940528,  0.01733442,  0.06339893,\n",
       "         -0.22060837, -0.16722457,  0.05380146, -0.04805036, -0.1027537 ,\n",
       "          0.03699827, -0.09889187,  0.22794246, -0.14632607, -0.04494045,\n",
       "         -0.07774521,  0.13874748, -0.0347735 , -0.13634162, -0.20190615,\n",
       "         -0.21641078,  0.31347254, -0.14486305, -0.03524852,  0.07201703,\n",
       "         -0.18046462, -0.18335087,  0.01819028, -0.06831815,  0.07935538,\n",
       "          0.18334789,  0.08771618, -0.15606775, -0.09551093,  0.02745223],\n",
       "        [-0.02575081,  0.04317146, -0.0177825 ,  0.02086985, -0.06232171,\n",
       "         -0.03008255, -0.07734696, -0.08613253,  0.04808907,  0.03822695,\n",
       "         -0.10061175,  0.1781011 , -0.04417665,  0.04423587, -0.01193318,\n",
       "          0.24874387, -0.01710671, -0.0714535 ,  0.0117582 ,  0.03947231,\n",
       "          0.02853592,  0.06139537,  0.01418836, -0.07444633,  0.02106781,\n",
       "          0.04914502,  0.11847721, -0.14160761,  0.05071671,  0.05056993,\n",
       "         -0.01019428, -0.0122616 ,  0.02060154,  0.04250023,  0.07596108,\n",
       "          0.11631959, -0.11163284,  0.06534414, -0.01616724,  0.00717828,\n",
       "          0.14085904,  0.11167419, -0.00178506, -0.00464791, -0.06126203,\n",
       "         -0.08379527, -0.13648781,  0.04215094,  0.01965487, -0.07360837],\n",
       "        [ 0.01875775, -0.05334946, -0.0012686 , -0.00277567,  0.02163978,\n",
       "          0.02887242,  0.02571989,  0.06430017, -0.0279285 , -0.04067869,\n",
       "          0.0926621 , -0.10496916,  0.04013125, -0.02481732,  0.01159901,\n",
       "         -0.17962791, -0.00655324,  0.07580301, -0.01637003, -0.05011128,\n",
       "          0.00595011, -0.05586117, -0.01680208,  0.07965708,  0.0159273 ,\n",
       "         -0.06873024, -0.09354213,  0.09086706,  0.00445558, -0.01544606,\n",
       "          0.01947488, -0.02346233, -0.00430444, -0.02419002, -0.06065446,\n",
       "         -0.07630517,  0.04834077, -0.06402318,  0.01963116, -0.05097338,\n",
       "         -0.10775496, -0.0989733 ,  0.01647159,  0.0079191 ,  0.03040752,\n",
       "          0.05692248,  0.12076453, -0.01701714,  0.01536656,  0.08622431],\n",
       "        [ 0.17929607,  0.23998702, -0.0007813 , -0.04627031, -0.21552547,\n",
       "          0.13087374, -0.02825453, -0.14795597,  0.0176924 ,  0.24209359,\n",
       "          0.24008706,  0.28121925,  0.0798151 , -0.18563039,  0.05615202,\n",
       "          0.15137791,  0.21379553,  0.17812842,  0.21522318,  0.05139865,\n",
       "          0.21268026,  0.13502104, -0.02239148,  0.25243159,  0.01659818,\n",
       "         -0.17773107, -0.37592373, -0.07239254, -0.23080101, -0.01970483,\n",
       "          0.09557231,  0.17835623, -0.17085395, -0.01833892,  0.28448348,\n",
       "          0.08316457, -0.21821661,  0.19001139, -0.00892275,  0.13934186,\n",
       "         -0.2071442 ,  0.14156045,  0.01428187,  0.11353179, -0.16915779,\n",
       "         -0.21762367,  0.1108321 ,  0.09219999,  0.08966884, -0.03526237],\n",
       "        [-0.13053749, -0.08243426, -0.04020771, -0.01075441,  0.06002184,\n",
       "         -0.04394569, -0.08304431,  0.0493212 ,  0.02775886, -0.12984819,\n",
       "         -0.21360608, -0.01014703, -0.05392855,  0.19334166, -0.07842016,\n",
       "          0.07065011, -0.09180461, -0.14144045, -0.10212046,  0.05961325,\n",
       "         -0.03892055,  0.02159938,  0.03428672, -0.16546624,  0.00157277,\n",
       "          0.1264874 ,  0.23367644,  0.00246734,  0.13899226,  0.03639182,\n",
       "         -0.05881105, -0.0950629 ,  0.09130982,  0.07216272, -0.10958484,\n",
       "          0.10419451,  0.11294202, -0.00994087,  0.02877406, -0.02270646,\n",
       "          0.25088803,  0.04614485,  0.00820684, -0.09514732,  0.04078788,\n",
       "          0.04363739, -0.19624058, -0.04902419, -0.13246155, -0.04072638],\n",
       "        [-0.0356006 ,  0.03974557, -0.06269599,  0.03740708, -0.12384775,\n",
       "         -0.03063699, -0.15389008, -0.10725585,  0.05116872,  0.03114794,\n",
       "         -0.05306046,  0.28288168, -0.03711674,  0.05962295, -0.01692817,\n",
       "          0.29652623, -0.02332309, -0.00773264, -0.00612244, -0.03718741,\n",
       "          0.13266353,  0.08929551, -0.02934648,  0.00942046,  0.078088  ,\n",
       "         -0.01881147,  0.08431293, -0.17259026,  0.11563873,  0.06788668,\n",
       "          0.03528863, -0.07174101,  0.03270839,  0.08099984,  0.11289944,\n",
       "          0.15662563, -0.21436504,  0.07163901,  0.01101421, -0.06263553,\n",
       "          0.13677307,  0.12015824,  0.00166401,  0.03247209, -0.09024149,\n",
       "         -0.13373822, -0.09261204,  0.09734323,  0.0785841 , -0.01468282],\n",
       "        [ 0.04750899, -0.04220939,  0.06759076, -0.02978001,  0.11186889,\n",
       "          0.03012728,  0.14550003,  0.08957289, -0.03743854, -0.01567653,\n",
       "          0.04122522, -0.25730406,  0.03120045, -0.06936717,  0.03004052,\n",
       "         -0.25100994,  0.00621994, -0.00611643,  0.01948277,  0.0474024 ,\n",
       "         -0.14555084, -0.1042317 ,  0.03814957, -0.02768069, -0.06908746,\n",
       "          0.02146339, -0.06419594,  0.1354879 , -0.10260485, -0.04628358,\n",
       "         -0.04797294,  0.06902113, -0.02722674, -0.08610503, -0.10571453,\n",
       "         -0.15293036,  0.18165843, -0.07468951, -0.01918894,  0.05167232,\n",
       "         -0.13307843, -0.11850862,  0.00970892, -0.03430052,  0.07476677,\n",
       "          0.1280459 ,  0.07977311, -0.09195752, -0.05516683, -0.00237213],\n",
       "        [-0.09895517, -0.10376662, -0.02671736,  0.06318168,  0.02057156,\n",
       "         -0.07119469, -0.08773829, -0.02055221, -0.01301716, -0.03952849,\n",
       "         -0.05910803,  0.04613804, -0.07921775,  0.08687424, -0.0397085 ,\n",
       "          0.08326546, -0.14848339, -0.01468854, -0.11454237, -0.08914588,\n",
       "          0.0017772 , -0.01390279, -0.02554068, -0.03737784,  0.06480419,\n",
       "          0.00330775,  0.17997411, -0.09841808,  0.19765008,  0.00845   ,\n",
       "          0.02158883, -0.18821364,  0.10743587,  0.05304747, -0.00976578,\n",
       "         -0.00887847, -0.08254633, -0.04484755,  0.01130831, -0.14924086,\n",
       "          0.09425417, -0.03027398, -0.01950912,  0.03273073,  0.05323584,\n",
       "          0.05971716,  0.0181345 ,  0.05371107,  0.0816444 ,  0.05591453]])}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.arange(vocab_size)\n",
    "X_test = np.expand_dims(X_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3, 4, 5, 6, 7]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(np.arange(vocab_size), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_test, _ = forward_propagation(X_test, paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_sorted_inds = np.argsort(softmax_test, axis=0)[-4:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 6, 0, 0, 1, 5],\n",
       "       [4, 4, 7, 7, 2, 7, 4, 3],\n",
       "       [3, 3, 6, 4, 6, 6, 3, 4],\n",
       "       [5, 5, 1, 0, 7, 1, 5, 2]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_sorted_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the's neighbor words: ['dog', 'fox', 'brown', 'lazy']\n",
      "lazy's neighbor words: ['dog', 'fox', 'brown', 'lazy']\n",
      "jumps's neighbor words: ['lazy', 'over', 'quick', 'the']\n",
      "fox's neighbor words: ['the', 'brown', 'quick', 'over']\n",
      "brown's neighbor words: ['quick', 'over', 'jumps', 'the']\n",
      "dog's neighbor words: ['lazy', 'over', 'quick', 'the']\n",
      "over's neighbor words: ['dog', 'fox', 'brown', 'lazy']\n",
      "quick's neighbor words: ['jumps', 'brown', 'fox', 'dog']\n"
     ]
    }
   ],
   "source": [
    "for input_ind in range(vocab_size):\n",
    "    input_word = id_to_word[input_ind]\n",
    "    output_words = [id_to_word[output_ind] for output_ind in top_sorted_inds[::-1, input_ind]]\n",
    "    print(\"{}'s neighbor words: {}\".format(input_word, output_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
