{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is to show how to train word embeddings using our own dataset. Alternatively we will show the training process for producing word embeddings using the word2vec, GloVe and fastText models. For this task we will use the STS BEnchmark dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data Loading and Preprocessing\n",
    "* Word2Vec\n",
    "* fastText\n",
    "* GloVe\n",
    "* Concluding Remarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd \n",
    "\n",
    "from nltk.corpus import brown\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.fasttext import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path for where your repo is located\n",
    "path = '/Users/cblanesg/cam.blanes Dropbox/Camila Blanes/deep_dive/ghost_recon/Embeddings/'\n",
    "NLP_REPO_PATH = os.path.join(path)\n",
    "\n",
    "# Set the path for where your datasets are located\n",
    "BASE_DATA_PATH = os.path.join(NLP_REPO_PATH, \"sts-train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps for pre processing the text are the following:\n",
    "1. Stemming\n",
    "2. Remove punctuation\n",
    "3. Everything to lowercase \n",
    "4. Remove stopwords\n",
    "5. **(Optional)** Remove words which appear less than 1% and more than 99% of documents in the corpus (Consider the diversity of vocabulary, average length of document and size of corpus). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec is a predictive model for learning word embeddings from text. Word embeddings are learned such that words that share common contexts in the corpus will be close together in the vector space. There are two different model architectures that can be used to produce word2vec embeddings: continuous bag-of-words (CBOW) or continuous skip-gram. The former uses a window of surrounding words (the \"context\") to predict the current word and the latter uses the current word to predict the surrounding context words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ['The', 'September-October', 'term', 'jury', 'had', 'been', 'charged', 'by', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.', '.']]\n"
     ]
    }
   ],
   "source": [
    "# We will use the same dataset used before to train the word embeddings from Word2Vec. \n",
    "sentences = brown.sents()\n",
    "print(sentences[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Word2Vec function has different parameters:\n",
    "1. size: length of the word embedding/vector (defaults to 100)\n",
    "2. window: maximum distance between the word being predicted and the current word (defaults to 5)\n",
    "3. min_count: ignores all words that have a frequency lower than this value (defaults to 5)\n",
    "4. workers: number of worker threads used to train the model (defaults to 3)\n",
    "5. sg: training algorithm; 1 for skip-gram and 0 for CBOW (defaults to 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIM = 100\n",
    "\n",
    "w2v = Word2Vec(sentences, \n",
    "               size = EMB_DIM, \n",
    "               window = 5,\n",
    "               min_count = 5,\n",
    "               workers = 3,\n",
    "               sg = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the program is trained we can:\n",
    "* Query for the word embeddings of a given word\n",
    "* Inspect the model vocabulary\n",
    "* Save the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding for election: [ 0.11844847 -0.11637932 -0.3091827  -0.33256492 -0.09560913  0.01062229\n",
      " -0.17380676 -0.02245147  0.09547108  0.04264915 -0.13025706 -0.3440462\n",
      "  0.03201969  0.14489679  0.00758509 -0.2599973  -0.30909446  0.1263442\n",
      " -0.28031054  0.39954695 -0.15825106 -0.01902786 -0.02355967  0.06687792\n",
      "  0.28980207 -0.07969534  0.03013778 -0.09673845 -0.11158438 -0.31007853\n",
      " -0.2609249  -0.08558761 -0.07878723 -0.09899204 -0.19056423  0.02196357\n",
      " -0.19999109 -0.09447739 -0.28249282 -0.14981844 -0.1258716  -0.40801698\n",
      "  0.20556849  0.11825003 -0.16992597  0.2365957  -0.44568986 -0.26205978\n",
      "  0.09935443 -0.01176151 -0.19523796  0.3195377   0.34194356  0.07939881\n",
      "  0.27538422 -0.29285547  0.03440086  0.20882723  0.08378489 -0.00295072\n",
      "  0.06728559 -0.04086252  0.132572    0.12622212 -0.00713637  0.04562614\n",
      " -0.15339862  0.10641158  0.16521294 -0.00504828  0.38302675 -0.09802999\n",
      " -0.23418489 -0.07284083  0.19840676  0.06116144 -0.17556016  0.16772768\n",
      "  0.12868421  0.01283603 -0.36153147 -0.06286982  0.06448457  0.04773733\n",
      "  0.18178569 -0.27980804  0.04354953 -0.06850497 -0.01429569 -0.02671591\n",
      "  0.13527483 -0.1390323   0.01582589 -0.23134772  0.3256647   0.14504127\n",
      "  0.4702302  -0.50280637  0.12871182  0.12751852]\n"
     ]
    }
   ],
   "source": [
    "# Query for the word embeddings of a given word\n",
    "print(\"embedding for election:\",  w2v.wv[\"election\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 30 vocabulary words: ['The', 'Fulton', 'County', 'Grand', 'said', 'Friday', 'an', 'investigation', 'of', 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities']\n"
     ]
    }
   ],
   "source": [
    "# Inspect the model vocabulary \n",
    "print(\"\\nFirst 30 vocabulary words:\", list(w2v.wv.vocab)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to Saturday:\n",
      " [('Monday', 0.9576911330223083), ('Sunday', 0.9552295207977295), ('Pennsylvania', 0.953289270401001), ('eighth', 0.9518365859985352), ('sixteenth', 0.9496550559997559), ('Easter', 0.9486106634140015), ('Silence', 0.9478686451911926), ('20th', 0.9454261660575867), ('ending', 0.9454016089439392), ('fourth', 0.9434164762496948)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Check most similar word \n",
    "print(\"Most similar to Saturday:\\n\", w2v.similar_by_word(\"Saturday\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the FastText model\n",
    "fastText_model = FastText(size=100, window=5, min_count=5, sentences=sentences, iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for election: [-3.0171871e-04  8.1126946e-01 -6.0227215e-01  5.8451658e-01\n",
      "  1.9119148e-01 -2.6985628e-03 -5.5760586e-01 -8.4915543e-01\n",
      " -1.2416487e-02 -4.1302424e-02  6.3710523e-01  4.5659903e-01\n",
      "  1.0093259e-01  1.2377757e+00  7.6508266e-01  9.5967108e-01\n",
      " -5.6427377e-01 -1.3122000e-01 -4.6205842e-01 -1.0577116e+00\n",
      "  3.2587388e-01  4.1558594e-01  3.2371905e-01  2.6745009e-01\n",
      " -5.0153751e-02 -5.7547307e-01 -6.6508061e-01 -4.6496564e-01\n",
      " -6.7831701e-01  2.6945293e-01 -5.6355432e-02 -8.7843937e-01\n",
      "  5.4984015e-01 -4.6731395e-01 -5.0314542e-02 -1.3001447e+00\n",
      " -8.4473205e-01 -6.3425086e-02 -6.3968778e-01  4.0601847e-01\n",
      " -9.1658586e-01  8.0501206e-02 -2.4565169e-01  4.2483443e-01\n",
      " -8.8506520e-01  1.8807220e-01 -4.1304886e-01  1.1536719e+00\n",
      "  1.4344645e-01 -1.8072079e-01 -8.5766412e-02  9.3326962e-01\n",
      " -4.4527104e-01 -1.9733871e+00  4.1668475e-01 -8.4575778e-01\n",
      "  6.8148452e-01  2.8996772e-01 -1.9695307e-01 -8.5368168e-01\n",
      "  1.4542532e+00  5.3738928e-01 -5.9601694e-01 -5.8495277e-01\n",
      "  5.1189619e-01 -2.0703821e-01 -1.3610066e-01 -4.1113892e-01\n",
      " -5.2519834e-01 -2.3283996e-01  7.2823977e-01  9.9335289e-01\n",
      "  3.1157954e-02  5.3959614e-01 -6.0561842e-01 -2.3898515e-01\n",
      "  8.1219810e-01 -3.9261708e-01  3.8246056e-01  5.8663011e-01\n",
      "  6.1293131e-01 -6.9923073e-01 -7.4196619e-01  8.6635870e-01\n",
      " -5.1762807e-01  6.5541029e-01 -4.0490413e-01 -2.3489223e+00\n",
      " -1.0464355e+00  1.2777673e+00 -6.7765921e-01 -7.0530915e-01\n",
      "  5.9158349e-01 -4.5295340e-01 -1.1807215e+00 -7.4755359e-01\n",
      "  1.8001090e-01  2.1399797e-01 -1.6335293e-03  5.0986820e-01]\n",
      "\n",
      "First 30 vocabulary words: ['The', 'Fulton', 'County', 'Grand', 'said', 'Friday', 'an', 'investigation', 'of', 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities']\n"
     ]
    }
   ],
   "source": [
    "# 1. Let's see the word embedding for \"apple\" by accessing the \"wv\" attribute and passing in \"apple\" as the key.\n",
    "print(\"Embedding for election:\", fastText_model.wv[\"election\"])\n",
    "\n",
    "# 2. Inspect the model vocabulary by accessing keys of the \"wv.vocab\" attribute. We'll print the first 20 words.\n",
    "print(\"\\nFirst 30 vocabulary words:\", list(fastText_model.wv.vocab)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training occurs on word-word co-occurrence statistics with the objective of learning word embeddings such that the dot product of two words' embeddings is equal to the words' probability of co-occurrence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
